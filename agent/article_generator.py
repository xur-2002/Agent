"""Minimalist article generation module for lowest cost closed loop.

Supports multiple LLM providers:
- groq: Free tier via OpenAI-compatible API, llama-3.1-8b-instant
- openai: OpenAI GPT-4o-mini (paid)
- dry_run: Mock article generation (free)
"""

import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import re

logger = logging.getLogger(__name__)


# ============================================================================
# Exception Types
# ============================================================================

class LLMProviderError(Exception):
    """Base exception for LLM provider issues."""
    def __init__(self, message: str, provider: str, retriable: bool = True):
        self.message = message
        self.provider = provider
        self.retriable = retriable
        super().__init__(f"[{provider}] {message}")


class MissingAPIKeyError(LLMProviderError):
    """API key not configured."""
    def __init__(self, provider: str):
        super().__init__(f"API key not configured", provider, retriable=False)


class InsufficientQuotaError(LLMProviderError):
    """API quota exhausted or billing issue."""
    def __init__(self, provider: str):
        super().__init__(f"Insufficient quota / billing issue", provider, retriable=False)


class RateLimitError(LLMProviderError):
    """Rate limit hit."""
    def __init__(self, provider: str):
        super().__init__(f"Rate limited", provider, retriable=True)


class TransientError(LLMProviderError):
    """Transient network/timeout error."""
    def __init__(self, provider: str, original_error: str):
        super().__init__(f"Network error: {original_error}", provider, retriable=True)


# ============================================================================
# LLM Provider Factory
# ============================================================================

def _get_llm_client(provider: str) -> Tuple[Any, str, bool]:
    """Get LLM client for a specific provider.
    
    Args:
        provider: groq, openai, or dry_run
        
    Returns:
        Tuple of (client, model, is_dry_run)
        
    Raises:
        MissingAPIKeyError: If API key not configured
    """
    from agent.config import Config
    
    if provider == "groq":
        api_key = Config.GROQ_API_KEY.strip()
        if not api_key:
            raise MissingAPIKeyError("groq")
        
        try:
            from openai import OpenAI
        except ImportError:
            raise LLMProviderError("openai package not installed", "groq", False)
        
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.groq.com/openai/v1"
        )
        model = Config.GROQ_MODEL
        logger.info(f"Initialized Groq client (model: {model})")
        return client, model, False
    
    elif provider == "openai":
        api_key = Config.OPENAI_API_KEY.strip()
        if not api_key:
            raise MissingAPIKeyError("openai")
        
        try:
            from openai import OpenAI
        except ImportError:
            raise LLMProviderError("openai package not installed", "openai", False)
        
        client = OpenAI(api_key=api_key)
        model = Config.OPENAI_MODEL
        logger.info(f"Initialized OpenAI client (model: {model})")
        return client, model, False
    
    elif provider == "dry_run":
        logger.info("Using DRY_RUN mock mode")
        return None, "mock", True
    
    else:
        raise ValueError(f"Unknown LLM provider: {provider}")


def slugify(text: str, max_length: int = 60) -> str:
    """Convert text to URL-safe slug.
    
    Args:
        text: Input text
        max_length: Maximum slug length
        
    Returns:
        URL-safe slug
    """
    text = text.lower().strip()
    text = re.sub(r'[^\w\s-]', '', text)
    text = re.sub(r'[-\s]+', '-', text)
    return text[:max_length].rstrip('-')


def generate_article(
    keyword: str,
    search_results: List[Dict[str, Any]] = None,
    dry_run: bool = False,
    language: str = "zh-CN",
    provider: Optional[str] = None
) -> Optional[Dict[str, Any]]:
    """Generate article with configurable LLM provider.
    
    Args:
        keyword: The keyword/topic
        search_results: List of search results with title, snippet, link
        dry_run: If True, use mock generation mode
        language: zh-CN or en-US
        provider: groq, openai, dry_run (if None, use Config.LLM_PROVIDER)
        
    Returns:
        Dict with article data including provider/model info, or None if failed
        
    Raises:
        LLMProviderError: On API errors (with retriable flag)
    """
    from agent.config import Config
    
    if search_results is None:
        search_results = []
    
    if provider is None:
        provider = Config.LLM_PROVIDER
    
    # Override with dry_run parameter
    if dry_run:
        provider = "dry_run"
    
    # Get client for provider
    try:
        client, model, is_dry_run = _get_llm_client(provider)
    except LLMProviderError:
        raise
    
    # Generate mock article for dry_run
    if is_dry_run:
        logger.info(f"[DRY_RUN] Generating mock article for keyword: {keyword}")
        article = _generate_mock_article(keyword, search_results)
        article["provider"] = "dry_run"
        article["model"] = "mock"
        return article
    
    # Prepare sources for context
    sources = []
    source_text = ""
    
    if search_results:
        for i, result in enumerate(search_results[:5], 1):
            title = result.get("title", "")
            snippet = result.get("snippet", "")
            link = result.get("link", "")
            sources.append({"title": title, "link": link})
            source_text += f"{i}. [{title}]({link})\n   {snippet}\n\n"
    
    # Build prompt
    if language == "zh-CN":
        if source_text:
            prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œä¸ºå…³é”®è¯"{keyword}"å†™ä¸€ç¯‡ 600-800 å­—çš„ä¸­æ–‡æ–‡ç« ã€‚

æœç´¢ç»“æœï¼š
{source_text}

è¦æ±‚ï¼š
1. æ–‡ç« ç»“æ„ï¼šæ ‡é¢˜ã€å¯¼è¯­ï¼ˆ100å­—å·¦å³ï¼‰ã€æ­£æ–‡ã€å°ç»“
2. å®Œå…¨åŸºäºæœç´¢ç»“æœçš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ æ•°æ®
3. å¦‚æœæ— æ³•ç¡®å®šçš„ä¿¡æ¯ï¼Œç”¨"æ®ç§°"ã€"æ®æŠ¥é“"æˆ–"æš‚æ— å…¬å¼€æ•°æ®"ç­‰æªè¾
4. æ–‡ç« æœ«å°¾å¿…é¡»åˆ—å‡º 3-5 ä¸ªå‚è€ƒé“¾æ¥ï¼ˆä»æœç´¢ç»“æœä¸­é€‰å–ï¼‰
5. ä½¿ç”¨ Markdown æ ¼å¼
6. ä¸“ä¸šã€å®¢è§‚çš„è¯­æ€

è¾“å‡ºæ ¼å¼ï¼ˆå°±æ˜¯è¿™ä¸ªæ ¼å¼ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼‰ï¼š
# [æ–‡ç« æ ‡é¢˜]

## å¯¼è¯­

[å¯¼è¯­å†…å®¹ï¼Œ100å­—å·¦å³]

## æ­£æ–‡

[æ­£æ–‡å†…å®¹ï¼Œ400-600å­—]"""
        else:
            prompt = f"""ä¸ºå…³é”®è¯"{keyword}"å†™ä¸€ç¯‡ 600-800 å­—çš„ä¸­æ–‡æ–‡ç« ã€‚

è¦æ±‚ï¼š
1. æ–‡ç« ç»“æ„ï¼šæ ‡é¢˜ã€å¯¼è¯­ï¼ˆ100å­—å·¦å³ï¼‰ã€æ­£æ–‡ã€å°ç»“
2. åŸºäºä¸€èˆ¬çŸ¥è¯†å’Œå¸¸è§è®¤çŸ¥è¿›è¡Œåˆ›ä½œ
3. ä½¿ç”¨ Markdown æ ¼å¼
4. ä¸“ä¸šã€å®¢è§‚çš„è¯­æ€

è¾“å‡ºæ ¼å¼ï¼ˆå°±æ˜¯è¿™ä¸ªæ ¼å¼ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼‰ï¼š
# [æ–‡ç« æ ‡é¢˜]

## å¯¼è¯­

[å¯¼è¯­å†…å®¹ï¼Œ100å­—å·¦å³]

## æ­£æ–‡

[æ­£æ–‡å†…å®¹ï¼Œ400-600å­—]

## å°ç»“

[å°ç»“ï¼Œ100å­—å·¦å³]"""
    else:
        if source_text:
            prompt = f"""Based on the following search results, write a 500-800 word English article about "{keyword}".

Search results:
{source_text}

Requirements:
1. Structure: Title, Introduction (80 words), Body, Conclusion
2. Based on search results only, no fabrication
3. For uncertain information, use "reportedly", "according to", etc.
4. Add 3-5 reference links
5. Use Markdown format
6. Professional and objective tone

Output format (exactly this format, nothing else):
# [Article Title]

## Introduction

[Introduction content, ~80 words]

## Body

[Main content, 300-500 words]

## Conclusion

[Conclusion, ~80 words]

## References

- [Link Title](URL)
- [Link Title](URL)
"""
        else:
            prompt = f"""Write a 500-800 word English article about "{keyword}".

Requirements:
1. Structure: Title, Introduction (80 words), Body, Conclusion
2. Based on general knowledge
3. Use Markdown format
4. Professional and objective tone

Output format (exactly this format, nothing else):
# [Article Title]

## Introduction

[Introduction content, ~80 words]

## Body

[Main content, 300-500 words]

## Conclusion

[Conclusion, ~80 words]"""
    
    # Call the LLM
    try:
        logger.info(f"Calling {provider} API for keyword: {keyword}")
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a professional editor writing factual, well-researched articles."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1200,
            timeout=30
        )
        
        content = response.choices[0].message.content.strip()
        logger.info(f"Article generated by {provider} for keyword: {keyword}")
        
        # Parse the response
        article = _parse_article_response(content)
        if article:
            article["keyword"] = keyword
            article["sources"] = sources
            article["provider"] = provider
            article["model"] = model
            article["word_count"] = len(content.split())
            article["sources_count"] = len(sources)
            return article
        else:
            logger.error("Failed to parse article response")
            return None
            
    except Exception as e:
        # Classify the exception
        error_str = str(e).lower()
        
        if "insufficient_quota" in error_str or "billing" in error_str or "quota" in error_str:
            raise InsufficientQuotaError(provider)
        elif "invalid_api_key" in error_str or "401" in error_str or "unauthorized" in error_str:
            raise MissingAPIKeyError(provider)
        elif "rate_limit" in error_str or "429" in error_str:
            raise RateLimitError(provider)
        elif "timeout" in error_str or "connection" in error_str or "network" in error_str:
            raise TransientError(provider, str(e))
        else:
            # Generic error
            logger.error(f"{provider} API call failed: {e}")
            raise LLMProviderError(f"API call failed: {str(e)[:100]}", provider, retriable=True)


def _generate_mock_article(
    keyword: str,
    search_results: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """Generate a mock article for DRY_RUN/testing mode.
    
    Args:
        keyword: The keyword/topic
        search_results: Search results (unused in mock)
        
    Returns:
        Dict with full article data
    """
    sources = []
    for i, result in enumerate(search_results[:5], 1):
        sources.append({
            "title": result.get("title", f"Source {i}"),
            "link": result.get("link", f"https://example.com/{i}")
        })
    
    title = f"Understanding {keyword}"
    
    markdown = f"""# {title}

## Introduction

This article explores {keyword} and its importance in today's world.
Understanding {keyword} is crucial for professionals and organizations.

## Body

{keyword} is a significant topic that requires careful consideration. Here are the key aspects:

1. **First Aspect**: {keyword} has grown significantly in importance
2. **Second Aspect**: Organizations are increasingly focusing on {keyword}
3. **Third Aspect**: The future of {keyword} depends on several factors

These factors demonstrate the relevance and importance of {keyword} in the current landscape.

## Conclusion

{keyword} continues to be an important area for development and innovation.
As the landscape evolves, stakeholders should stay informed about the latest developments.

## References

"""
    
    for source in sources:
        markdown += f"- [{source['title']}]({source['link']})\n"
    
    return {
        "title": title,
        "body": markdown,
        "keyword": keyword,
        "sources": sources,
        "provider": "dry_run",
        "model": "mock",
        "word_count": sum(len(line.split()) for line in markdown.split('\n')),
        "sources_count": len(sources)
    }


def _parse_article_response(content: str) -> Optional[Dict[str, str]]:
    """Parse article response from LLM.
    
    Args:
        content: Raw response from LLM
        
    Returns:
        Dict with title and body, or None if parsing failed
    """
    try:
        # Extract title (first # heading)
        title_match = re.search(r'^#+\s*(.+?)$', content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else "Article"
        
        # Entire content is the body
        body = content.strip()
        
        return {
            "title": title,
            "body": body
        }
    except Exception as e:
        logger.error(f"Failed to parse article response: {e}")
        return None


def save_article(
    article: Dict[str, Any],
    output_dir: str = "outputs/articles"
) -> Optional[str]:
    """Save article to disk in outputs/articles/YYYY-MM-DD/<slug>.*
    
    Args:
        article: Article dict with provider, model, keyword, sources, etc.
        output_dir: Base output directory
        
    Returns:
        Path to saved markdown file, or None if failed
    """
    try:
        # Create date-based directory
        today = datetime.now().strftime("%Y-%m-%d")
        article_dir = Path(output_dir) / today
        article_dir.mkdir(parents=True, exist_ok=True)
        
        # Create slug from title
        slug = slugify(article.get("title", article.get("keyword", "article")))
        
        # Save markdown
        md_path = article_dir / f"{slug}.md"
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(article["body"])
        logger.info(f"Saved article markdown: {md_path}")
        
        # Save JSON metadata with provider info
        metadata = {
            "title": article.get("title", ""),
            "keyword": article.get("keyword", ""),
            "provider": article.get("provider", "unknown"),
            "model": article.get("model", "unknown"),
            "sources": article.get("sources", []),
            "created_at": datetime.now().isoformat(),
            "word_count": article.get("word_count", 0),
            "sources_count": article.get("sources_count", 0),
            "file_path": str(md_path)
        }
        json_path = article_dir / f"{slug}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"Saved article metadata: {json_path}")
        
        return str(md_path)
        
    except Exception as e:
        logger.error(f"Failed to save article: {e}")
        return None


def generate_article_from_material(keyword: str, material_pack: Dict[str, Any], language: str = 'zh-CN') -> Dict[str, Any]:
    """High-level generator: try LLM, fallback to template when unavailable.

    material_pack: {'sources': [{'title','link','snippet',...}], 'key_points': [...]}
    Returns article dict compatible with save_article()
    """
    from agent.config import Config
    search_results = material_pack.get('sources', [])

    providers = []
    if getattr(Config, 'LLM_PROVIDER', None):
        providers.append(Config.LLM_PROVIDER)
    if Config.OPENAI_API_KEY and 'openai' not in providers:
        providers.append('openai')

    # Try providers
    for p in providers:
        try:
            art = generate_article(keyword=keyword, search_results=search_results, dry_run=(p=='dry_run'), language=language, provider=p)
            if art:
                art['fallback_used'] = False
                return art
        except Exception as e:
            logger.warning(f"Provider {p} failed for keyword {keyword}: {e}")
            continue

    # Fallback template
    title = f"{keyword} â€” æ·±åº¦è§£è¯»"
    key_points = material_pack.get('key_points') or []
    sources = material_pack.get('sources') or []
    summary = (key_points[0] if key_points else f"å…³äº {keyword} çš„æ¦‚è¦ä»‹ç»ã€‚")

    body = f"# {title}\n\n## å¯¼è¯­\n\n{summary}\n\n## æ­£æ–‡\n\n"
    if key_points:
        for kp in key_points[:6]:
            body += f"- {kp}\n"
    else:
        body += f"{keyword} æ˜¯å½“å‰å…³æ³¨çš„è¯é¢˜ï¼Œä»¥ä¸‹ä¸ºåŸºç¡€æ¦‚è¿°ä¸èƒŒæ™¯ä¿¡æ¯ã€‚\n"

    body += "\n## å‚è€ƒæ¥æº\n\n"
    for s in sources[:5]:
        t = s.get('title') or s.get('link')
        l = s.get('link')
        body += f"- [{t}]({l})\n"

    return {
        'title': title,
        'body': body,
        'keyword': keyword,
        'sources': [{'title': s.get('title',''), 'link': s.get('link','')} for s in sources],
        'provider': 'none',
        'model': 'none',
        'word_count': len(body.split()),
        'sources_count': len(sources),
        'fallback_used': True
    }


def generate_article_in_style(
    keyword: str,
    material_pack: Dict[str, Any],
    style: str = 'wechat',
    word_count_range: tuple = (800, 1200),
    language: str = 'zh-CN'
) -> Dict[str, Any]:
    """Generate article in specific style (wechat or xiaohongshu).
    
    Args:
        keyword: Topic/keyword
        material_pack: {'sources': [...], 'key_points': [...]}
        style: 'wechat' (800-1200 words) or 'xiaohongshu' (300-600 words, casual)
        word_count_range: (min_words, max_words) tuple
        language: zh-CN or en-US
        
    Returns:
        Dict with article content and metadata
    """
    from agent.config import Config
    search_results = material_pack.get('sources', [])
    
    min_words, max_words = word_count_range
    
    # Build style-specific prompt
    providers = []
    if getattr(Config, 'LLM_PROVIDER', None):
        providers.append(Config.LLM_PROVIDER)
    if Config.OPENAI_API_KEY and 'openai' not in providers:
        providers.append('openai')
    
    if language == 'zh-CN':
        if style == 'wechat':
            style_desc = f"å…¬ä¼—å·ç±»å‹çš„æ–‡ç« ï¼Œç»“æ„å®Œæ•´ï¼ˆæ ‡é¢˜ã€å¯¼è¯­ã€åˆ†æ®µã€å°ç»“/é‡‘å¥ï¼‰ï¼Œ{min_words}-{max_words}å­—"
            style_prompt = """è¾“å‡ºæ ¼å¼ï¼š
# [æ–‡ç« æ ‡é¢˜]

## å¯¼è¯­

[å¯¼è¯­ï¼Œ100-150å­—]

## æ­£æ–‡

[åˆ†æ®µæ­£æ–‡ï¼Œ800-1000å­—]

## é‡‘å¥/å°ç»“

[é‡‘å¥æˆ–å°ç»“ï¼Œ80-120å­—]"""
        else:  # xiaohongshu
            style_desc = f"å°çº¢ä¹¦ç¬”è®°ç±»å‹ï¼Œå£è¯­é£æ ¼ã€ç§è‰/ç»éªŒå¸–ç»“æ„ï¼Œ{min_words}-{max_words}å­—ï¼Œæœ€åæœ‰äº’åŠ¨å¼•å¯¼"
            style_prompt = """è¾“å‡ºæ ¼å¼ï¼š
# [å¸å¼•çœ¼çƒçš„æ ‡é¢˜/ä¸»é¢˜]

[å¼€åœº/èƒŒæ™¯ï¼Œ30-50å­—]

## ğŸ“Œ æ ¸å¿ƒè¦ç‚¹

- [è¦ç‚¹1ï¼Œ20-40å­—]
- [è¦ç‚¹2ï¼Œ20-40å­—]
- [è¦ç‚¹3ï¼Œ20-40å­—]

## ğŸ’¡ ä¸ªäººå»ºè®®

[å®ç”¨å»ºè®®æˆ–ä½“éªŒåˆ†äº«ï¼Œ100-150å­—]

## â“ ä½ æ€ä¹ˆçœ‹ï¼Ÿ

[äº’åŠ¨å¼•å¯¼ï¼Œ20-40å­—]"""
        
        sources_text = ""
        for i, s in enumerate(search_results[:3], 1):
            title = s.get('title', '')
            snippet = s.get('snippet', '')
            link = s.get('link', '')
            sources_text += f"{i}. [{title}]({link})\n   {snippet}\n\n"
        
        if sources_text:
            prompt = f"ä¸ºå…³é”®è¯ \"{keyword}\" å†™ä¸€ç¯‡{style_desc}ã€‚\n\næœç´¢ç»“æœå‚è€ƒï¼š\n{sources_text}\n\nè¦æ±‚ï¼š\n1. åŸºäºæœç´¢ç»“æœçš„ä¿¡æ¯\n2. è¯­è¨€ç”ŸåŠ¨è‡ªç„¶\n3. ä½¿ç”¨ Markdown æ ¼å¼\n{style_prompt}"
        else:
            prompt = f"ä¸ºå…³é”®è¯ \"{keyword}\" å†™ä¸€ç¯‡{style_desc}ã€‚\n\nè¦æ±‚ï¼š\n1. åŸºäºå¸¸è§çŸ¥è¯†\n2. è¯­è¨€ç”ŸåŠ¨è‡ªç„¶\n3. ä½¿ç”¨ Markdown æ ¼å¼\n{style_prompt}"
    else:  # English
        if style == 'wechat':
            style_desc = f"Professional blog post, {min_words}-{max_words} words"
        else:
            style_desc = f"Casual social media post, {min_words}-{max_words} words, informal tone"
        prompt = f"Write a {style_desc} about \"{keyword}\".\n\nRequirements:\n1. Use Markdown format\n2. Professional and engaging\n\n# [Title]\n\n## Introduction\n\n[content]\n\n## Body\n\n[content]"
    
    # Try providers
    for p in providers:
        try:
            logger.debug(f"Generating {style} article for '{keyword}' using provider {p}")
            art = generate_article(keyword=keyword, search_results=search_results, dry_run=(p=='dry_run'), language=language, provider=p)
            if art:
                art['fallback_used'] = False
                art['style'] = style
                return art
        except Exception as e:
            logger.warning(f"Provider {p} failed for {style} article '{keyword}': {e}")
            continue
    
    # Fallback template
    if style == 'xiaohongshu':
        body = f"# {keyword}\n\n{keyword} æ˜¯å½“ä¸‹çš„çƒ­è¯é¢˜ã€‚\n\n## ğŸ“Œ æ ¸å¿ƒè¦ç‚¹\n\n"
        key_points = material_pack.get('key_points', [])
        for kp in (key_points[:3] if key_points else [f"å…³äº{keyword}çš„æ–°ä¿¡æ¯", f"{keyword}çš„ç°çŠ¶åˆ†æ"]):
            body += f"- {kp}\n"
        body += f"\n## ğŸ’¡ ä¸ªäººè§‚ç‚¹\n\nå…³äº{keyword}ï¼Œè¿™æ˜¯ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„è¯é¢˜ã€‚\n\n## â“ ä½ æ€ä¹ˆçœ‹ï¼Ÿ\n\næ¬¢è¿åˆ†äº«ä½ çš„çœ‹æ³•ï¼"
    else:  # wechat
        title = f"{keyword} â€” è¯¦è§£"
        key_points = material_pack.get('key_points') or []
        body = f"# {title}\n\n## å¯¼è¯­\n\n{keyword} æ˜¯å½“å‰çš„çƒ­é—¨è¯é¢˜ï¼Œæˆ‘ä»¬æ¥æ·±åº¦è§£è¯»ä¸€ä¸‹ã€‚\n\n## æ­£æ–‡\n\n"
        if key_points:
            for kp in key_points[:6]:
                body += f"- {kp}\n"
        else:
            body += f"å…³äº{keyword}çš„è¯¦ç»†åˆ†æå†…å®¹ã€‚\n"
        body += f"\n## æ€»ç»“\n\n{keyword}çš„é‡è¦æ„ä¹‰å’Œå‘å±•è¶‹åŠ¿ã€‚"
    
    return {
        'title': keyword,
        'body': body,
        'keyword': keyword,
        'sources': [{'title': s.get('title',''), 'link': s.get('link','')} for s in search_results],
        'provider': 'none',
        'model': 'none',
        'style': style,
        'word_count': len(body.split()),
        'sources_count': len(search_results),
        'fallback_used': True
    }

